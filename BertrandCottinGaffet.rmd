---
title: "Stability selection pour la prédiction de la résistance d'une souche bactérienne à un antibiotique"
subtitle: "M1 parcours SSD -- UE Apprentissage Statistique I"

date: "29/04/2022"

author:
  \begin{tabular}{ccc}
  Vadim \textsc{Bertrand} & Lola \textsc{Cottin} & Marie \textsc{Gaffet}
  \end{tabular}

output:
  pdf_document:
    number_sections: true
    citation_package: biblatex

bibliography: stabsel.bib

urlcolor: blue

header-includes:
   - \usepackage{caption}
   - \usepackage{longtable}
   - \usepackage{booktabs}
   - \usepackage{subcaption}
   - \usepackage{float}
   - \usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
   - \makeatletter\renewcommand*{\fps@figure}{H}\makeatother
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = F, message = F, warning = F, results = F)
knitr::opts_chunk$set(fig.width = 8, fig.height = 3, fig.align = "center")
```

```{r, lib}
library(ggpubr)
library(grid)
```

# Introduction

Certaines souches bactériennes présentent une résistance particulière aux antibiotiques développés pour lutter contre elles. \
Cette résistance peut être modélisée par une régression logistique ayant pour variables explicatives la présence ou non de motifs génomiques dans le génome des souches.
Seulement, le grand nombre de motifs (plusieurs dizaines de milliers) impose de sélectionner des variables explicatives avant d'entrainer le modèle de régression logistique.
Pour cela, des méthodes de régression pénalisant le nombre de variables sélectionnées existent.
L'approche du Lasso peut notamment être utilisée, mais quand les variables mises en jeu sont corrélées, elle présente l'inconvénient d'être instable et de sélectionner plus de variables que nécessaire. \
Pour pallier ce problème, la technique de stability selection \cite{stabsel} peut être intéressante.
Il s'agit de répéter plusieurs fois l'ajustement d'une régression logistique Lasso basée sur un sous-échantillonnage du jeu de données et d'en déduire la fréquence de sélection des variables par l'ensemble des régressions.
Les variables dont la fréquence de sélection dépasse un seuil donné sont enfin utilisées pour l'ajustement d'une régression logistique non pénalisée. \
Nous verrons ici comment implémenter la procédure de stability selection sur R, puis nous commenterons les résultats obtenus selon le seuil de sélection en les comparant à ceux d'une régression logistique Lasso "classique".

# Implémentation de la stability selection

```{r, source}
source("stabsel_func.R")
source("stabsel_data.R")
source("stabsel_fig.R")
```

L'ensemble du code mis en oeuvre pour l'implémentation et l'application de la stability selection est organisé en quatre fichiers :

  * ***stabsel_func.R*** contient les fonctions relatives à la stability selection (ajustement et évaluation) ;
  * ***stabsel_data.R*** pour importer le jeu de données utilisé ;
  * ***stabsel_run.R*** entraîne et évalue l'ensemble des modèles, puis sauvegarde un résumé des résultats ;
  * ***stabsel_fig.R*** définie les fonctions utiles pour l'affichage des résultats.

Le code étant présent et commenté en annexes, nous ne reviendrons donc que brièvement sur les éléments-clés :

  * nous nous sommes appuyés sur le package __R__ _glmnet_ \cite{glmnet} pour ajuster les régressions logistiques Lasso \cite{lasso} ;
  * par défaut, les sous-échantillons sélectionnés aléatoirement sont de taille $N/2$ avec $N$ la taille de l'échantillon ;
  * les modèles Lasso et sous-échantillons utilisés pour construire le chemin de stabilité sont au nombre de 100 ;
  * le maximum des fréquences, ou probabilités, de sélection selon les valeurs de lambda du chemin de stabilité est utilisé pour sélectionner les variables stables selon le seuil de stabilité souhaité ;
  * les seuils de stabilité considérés sont : ${0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1}$ ;
  * nous avons également implémenté la procédure de validation croisée pour ajuster l'hyper-paramètre du seuil de stabilité :
    * la procédure utilise 10 folds par défaut ;
    * deux seuils de stabilité optimaux sont retournés : l'un minimisant l'erreur, l'autre minimisant le nombre de variables tel que l'erreur se situe à un écart type du minimum ;
    * l'erreur utilisée est $100 - t_{agrément}$ avec $t_{agrément}$ le taux d'agrément, ou taux de bonne classification global.

# Prédiction de la résistance d'une souche bactérienne à un antibiotique

```{r, load}
load("data/summaries.Rdata")
load("data/paths.Rdata")
mods_summary <- rbind(lasso_summary, stab_sel_summary)
mods_summary$nzero <- as.integer(mods_summary$nzero)
```

Le jeu de donnée d'apprentissage que nous avons utilisé permet d'associer la résistance à la _streptomycine_ de 966 souches bactériennes de l'espèce _Mycobacterium tuberculosis_ à la présence de 53677 motifs génomiques \cite{data}.
Nous avons ensuite utilisé un jeu de test correspondant de 200 souches bactériennes. \
Les jeux de données ne présentant pas de données manquantes, la seule transformation que nous avons effectuée est l'encodage de la résistance à l'antibiotique de 1 pour résistant et -1 pour sensible à 1 pour résistant et 0 pour sensible.

La seconde étape de notre étude consistait à obtenir le chemin de régularisation et les hyper-paramètres $\lambda$ d'un modèle de régression logistique Lasso, puis le chemin de stabilité pour ces mêmes $\lambda$ du modèle de stability selection.
Nous avons représenté ces chemins sur la ___Figure 1___, en colorant chacune des variables selon le maximum pris par leur probabilité de sélection en fonction de $\lambda$. 

```{r, fig_chemins, fig.cap = "Chemins de stabilité (à gauche) et de régularisation (à droite)"}
fig_stab <- get_fig_stab(stab_path)
fig_reg <- get_fig_reg(reg_path)
ggarrange(fig_stab, fig_reg, ncol = 2)
```

Tout d'abord nous pouvons observer que pour les $\lambda$ d'index 25 à 100, la probabilité de sélection des variables les plus "importantes" varie peu.
Cette propriété mise en avant par Bühlmann et Meinshausen dans leur article \cite{stabsel} introduisant la stability selection conforte le choix du maximum des probabilités de sélection des variables comme statistique de sélection des variables selon le seuil de stabilité souhaité et permet de contourner la question parfois difficile du choix de l'hyper-paramètre $\lambda$ dans l'approche Lasso classique. \
Nous pouvons également remarquer que lorsque l'index de l'hyper-paramètre $\lambda$ est supérieur à 35, de nombreuses variables instables sont introduites dans le modèle Lasso classique, ce qui augmente inutilement la dimension de son support ; à l'inverse quand $\lambda$ est plus grand (pour des index inférieur à 35), la sélection des variables devient trop drastique et élimine des variables stables.

Nous avons ensuite souhaité observer si cette sélection de variables plus stable se traduit par l'ajustement de modèles de régression logistique plus performants.
Pour cela, nous avons calculé le taux de bonne de classification de dix modèles : le Lasso classique et neuf modèles de stability selection à des seuils de stabilité différents.
Etant donné que le nombre de variables mises en jeu dans les modèles est un facteur important, en particulier pour expliquer la résistance aux antibiotiques et pas uniquement la prédire, nous avons illustré sur la ___Figure 2___ la performance de chacun des modèles en fonction de la taille de leur support.

```{r, fig_perf, fig.height = 6, fig.cap = "Taux d'agrément en fonction de la taille du support (en haut) et courbes ROC (en bas) des différents modèles"}
fig_nz_sco <- get_fig_nz_score(mods_summary) +
  scale_x_continuous(breaks = c(seq(0, 25, 5), seq(50, 100, 25)),
                     labels = c(0, rep("", 4), seq(25, 100, 25))) +
  scale_y_continuous(breaks = c(seq(73, 89, 1)),
                     labels = c(rep("", 2), 75, rep("", 4), 80, rep("", 4), 85, 
                                rep("", 4)))
mods_roc <- get_rocs(X.train, y.train, X.test, y.test, mods_summary)
fig_roc <- get_fig_roc(mods_roc)
ggarrange(fig_nz_sco, fig_roc, nrow = 2, common.legend = TRUE, legend = "right")
```

Il est visible que le taux d'agrément de sept des modèles de stability selection est comparable à celui du Lasso (et meilleur pour quatre d'entre eux), tout en divisant par cinq, voire dix, le nombre de variables utilisées. \
Comme énoncé par Bühlmann et Meinshausen \cite{stabsel}, les résultats obtenus en terme de performance ou de taille du support sont proches pour des seuils de stabilité compris entre $0.6$ et $0.9$. Au delà de $0.9$, les performances semblent se dégrader fortement en l'absence de variables visiblement importantes. \
La similitude des résultats pour un seuil de stabilité cohérent et la facilité via ce genre de représentation à choisir ce seuil rendent l'utilisation de la validation croisée peu pertinente, d'autant que celle-ci est très coûteuse.
Par ailleurs, les sous-échantillonnages impliqués par l'approche de stability selection et la construction des probabilités de sélection jouent déjà un rôle similaire à celui de la validation croisée. \
S'attarder sur les courbes ROC des modèles peut également être intéressant pour observer leur différence avec un classifieur aléatoire.
Le constat est semblable à celui fait via le taux d'agrément : les modèles de stability selection pour des seuils de stabilité dans l'intervalle $[0.6 ,0.9]$ sont les plus proches du classifieur parfait.

## Discussion sur ces bons résultats

Pour conclure cette étude, nous avons tenté de comprendre pourquoi la stability selection semble bien adaptée à la problèmatique de la résistance aux antibiotiques. \
Nous nous sommes donc intéressés aux coefficients des différentes régression logistiques évoquées précédemment.
Pour rappel, dans le cadre de la régression logistique avec des prédicteurs binaires, les coefficients des modèles indiquent la variation du logarithme de la probabilité d'avoir l'évènement prédit (ici, la résistance à l'antibiotique) quand le prédicteur passe de $FAUX$ à $VRAI$ (ici, de motif génomique absent à présent).

La ___Figure 3___ illustre la répartition des coefficients des différents modèles pour plusieurs niveaux de significativité.

```{r, fig_coef, fig.height = 6, fig.cap = "Boîtes à moustaches des coefficients. Sans restiction (en haut à gauche) ; aux niveaux 0.01 (en haut à droite), 0.05 (en bas à gauche) et 0.001 (en bas à droite)"}
fig_coefs <- get_fig_coefs(mods_summary) +
  scale_y_continuous(limits = c(-25, 25))
fig_signif_coefs_01 <- get_fig_signif_coefs(mods_summary, 0.05) +
  scale_y_continuous(limits = c(-25, 25))
fig_signif_coefs_05 <- get_fig_signif_coefs(mods_summary, 0.01) +
  scale_y_continuous(limits = c(-25, 25))
fig_signif_coefs_001 <- get_fig_signif_coefs(mods_summary, 0.001) +
  scale_y_continuous(limits = c(-25, 25))
ggarrange(fig_coefs, fig_signif_coefs_01, fig_signif_coefs_05, 
          fig_signif_coefs_001, ncol = 2, nrow = 2, common.legend = TRUE, 
          legend = "right")
```

Nous constatons que les coefficients obtenus par stability selection (quelque soit le seuil de stabilité) sont majoritairement positifs tandis que les coefficients du Lasso classique sont positifs et négatifs. \
Nous pouvons donc en déduire que les modèles de stability selection retournent les motifs génomiques résistants à l'antibiotique (coefficients positifs) alors que le modèle Lasso classique retourne les motifs génomiques résistants et sensibles (coefficients négatifs) à l'antibiotique. 
La nature de la stability selection semble suggérer que les différents motifs sensibles à l'antibiotique sont moins récurrents dans les souches bactériennes, à l'inverse des motifs résistants. Il faudrait donc que le nombre de motifs sensibles différents connus du modèle ajusté soit supérieur au nombre de motifs 
résistants différents connus pour que la potentielle sensibilité de nouvelles souches soit captée à l'utilisation du modèle.
```{r, fig_mosaic, fig.height = 6, fig.cap = "Diagrammes en mosaique de la résistance antibiotique en fonction de la présence de motifs génomiques. En haut pour le modèle Lasso et en bas pour le stability selection au seuil 0.9. A gauche le motif dont la valeur absolue du coefficient est la plus faible et à droite la plus élevée"}
lasso_coefs <- get_coefs_df(mods_summary["Lasso", ], 0.05)
lasso_min_idx <- which.min(abs(lasso_coefs$coef))
lasso_min_var <- lasso_coefs$vars.idx[[lasso_min_idx]]
lasso_max_idx <- which.max(abs(lasso_coefs$coef))
lasso_max_var <- lasso_coefs$vars.idx[[lasso_max_idx]]

fig_lasso_min <-
  get_mosaic(X.train, y.train, lasso_min_var, lasso_coefs$coef[[lasso_min_idx]],
             "Lasso")
fig_lasso_max <-
  get_mosaic(X.train, y.train, lasso_max_var, lasso_coefs$coef[[lasso_max_idx]],
             "Lasso")

stab_sel_coefs <- get_coefs_df(mods_summary["StabSel-0.9 (cv_1sd)", ], 0.05)
stab_sel_min_idx <- which.min(abs(stab_sel_coefs$coef))
stab_sel_min_var <- stab_sel_coefs$vars.idx[[stab_sel_min_idx]]
stab_sel_max_idx <- which.max(abs(stab_sel_coefs$coef))
stab_sel_max_var <- stab_sel_coefs$vars.idx[[stab_sel_max_idx]]

fig_stab_sel_min <-
  get_mosaic(X.train, y.train, stab_sel_min_var, 
             stab_sel_coefs$coef[[stab_sel_min_idx]], 
             "Stability selection (0.9)")
fig_stab_sel_max <-
  get_mosaic(X.train, y.train, stab_sel_max_var, 
             stab_sel_coefs$coef[[stab_sel_max_idx]], 
             "Stability selection (0.9)")

fig <- ggarrange(fig_lasso_min + rremove("ylab") + rremove("xlab"), 
                 fig_lasso_max + rremove("ylab") + rremove("xlab"),
                 fig_stab_sel_min + rremove("ylab") + rremove("xlab"),
                 fig_stab_sel_max + rremove("ylab") + rremove("xlab"),
                 ncol = 2, nrow = 2, common.legend = TRUE, legend = "right")
annotate_figure(fig, left = textGrob("Souche", rot = 90, vjust = 1),
                bottom = textGrob("Motif génomique"))
```

De plus, nous remarquons que l'amplitude des coefficients du Lasso classique est très forte, y compris en se restreignant uniquement aux coefficients significatifs, ce qui est nettement moins le cas pour les modèles de stability selection.
Cela peut également s'expliquer par la tendance du Lasso classique à capter des motifs résistants et sensibles revenant peu régulièrement dans le jeu d'entraînement. \
Certains de ces motifs rares peuvent se montrer de bons classifieurs et la maximisation de la $log-vraisemblance$ qui est faite pour estimer les coefficients de la regression logistique va tirer les coefficients de ces classifieurs en négatifs pour les motifs sensibles, et en positifs pour les motifs résistants.
Ce phénomène se retrouve moins dans les modèles de stability selection car en conservant les variables les plus fréquentes il est immaginable que, prises séparément, ces variables soient des classifieurs plus équivalent, et donc que leurs coefficients restent plus proches de zéro.
La ___Figure 4___ présentant les tables de contingence de la présence de certains motifs génomiques et de la résistance à l'antibiotique vient illustrer ces suppositions.

La capacité des modèles de stability selection à capter la résistance plutôt que la sensibilité expliquerait donc les bons résultats prédictifs donnés par celle-ci malgré des supports nettement plus compacts. 
Dans l'optique d'expliquer la résistance aux antibiotiques plus que de la prédire il n'est pas nécessairement intéressant de limiter l'amplitude des coefficients, en revanche le nombre de variables à considérer reste un critère majeur.

# Conclusion

Blabla

# Annexes

***stabsel_func.R***

```{r, code_func, code = readLines("stabsel_func.R", encoding = "UTF-8"), echo = T, eval = F}
```

***stabsel_data.R***

```{r, code_data, code = readLines("stabsel_data.R", encoding = "UTF-8"), echo = T, eval = F}
```

***stabsel_run.R***

```{r, code_run, code = readLines("stabsel_run.R", encoding = "UTF-8"), echo = T, eval = F}
```

***stabsel_fig.R***

```{r, code_run, code = readLines("stabsel_fig.R", encoding = "UTF-8"), echo = T, eval = F}
```

# Références
