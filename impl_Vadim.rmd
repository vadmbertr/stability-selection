---
title: "Mise en oeuvre"
output: html_notebook
---

```{r}
library(ggplot2)
library(ggpubr)
library(glmnet)
library(pROC)
library(purrr)
```

```{r, dataset}
load("data/TB.Rdata")
# transformation de la réponse en 0 / 1
y.train <- (y.train + 1) / 2
y.test <- (y.test + 1) / 2
```

```{r, eval_func}
# retourne un modèle de regression logistique entrainé
get_glm <- function(X_train, y_train, vars_idx) {
  return(glm("y ~ .", data = cbind(as.data.frame(as.matrix(X_train[, vars_idx])), data.frame(y = y_train)),
             family = "binomial"))
}
# retourne les probabilités des classes
get_predictions <- function(X_train, y_train, X_test, vars_idx) {
  mod <- get_glm(X_train, y_train, vars_idx)
  return(predict(mod, newdata = as.data.frame(as.matrix(X_test[, vars_idx])), type = "response"))
}

# retourne le taux d'agrément
get_score <- function(y_test, y_pred) {
  return(sum(y_test == y_pred) / length(y_test) * 100)
}

# entraine un modèle de régression logistique non pénalisé (avec un jeu restreint aux variables stables)
# estime les classes
# retourne le taux d'agrément
get_performance <- function(X_train, y_train, X_test, y_test, vars_idx) {
  y_pred <- as.integer(get_predictions(X_train, y_train, X_test, vars_idx) > .5)
  return(get_score(y_test, y_pred))
}

# entraine un modèle de régression logistique non pénalisé (avec un jeu restreint aux variables stables)
# retourne l'amplitude des coefficients
get_coef_range <- function(X_train, y_train, vars_idx) {
  mod <- get_glm(X_train, y_train, vars_idx)
  return(max(mod$coefficients, na.rm = T) - min(mod$coefficients, na.rm = T))
}
get_coef_IQR <- function(X_train, y_train, vars_idx) {
  mod <- get_glm(X_train, y_train, vars_idx)
  return(quantile(mod$coefficients, .75, na.rm = T) - quantile(mod$coefficients, .25, na.rm = T))
}
get_coef_IDR <- function(X_train, y_train, vars_idx) {
  mod <- get_glm(X_train, y_train, vars_idx)
  return(quantile(mod$coefficients, .9, na.rm = T) - min(mod$coefficients, .1, na.rm = T))
}

# entraine un modèle de régression logistique non pénalisé (avec un jeu restreint aux variables stables)
# estime les probabilités des classes
# retourne la courbe ROC
get_roc <- function(X_train, y_train, X_test, y_test, vars_idx) {
  y_pred <- get_predictions(X_train, y_train, X_test, vars_idx)
  return(roc(y_test, y_pred))
}
```

```{r, stab_sel_func}
# retourne une matrice de 0 / 1 indiquant si le coefficient d'une variable est non nul pour un lambda donné
get_selected_vars <- function(X_train, y_train, lambda, sample_size_coef) {
  full_size <- nrow(X_train)
  fold_idx <- sample(full_size, full_size * sample_size_coef)
  X_fold <- X_train[fold_idx, ]
  y_fold <- y_train[fold_idx]
  return(glmnet(X_fold, y_fold, family = "binomial", lambda = lambda)$beta != 0)
}

# retourne un chemin de stabilité
get_stability_path <- function(X_train, y_train, lambda, sample_size_coef = .5) {
  n_paths <- 100
  # liste de n_models matrices de 0 / 1 indiquant la sélection ou non d'une variable pour un lambda donné
  vars_select <- sapply(1:n_paths, function(i) get_selected_vars(X_train, y_train, lambda, sample_size_coef))
  # passage à une matrice de fréquences des variables (pour des lambda différents)
  return(reduce(vars_select, `+`) / n_paths)
}

# retourne un data.frame donnant le score et la taille du support selon le seuil de stabilité
run_stability_selection_model <- function(X_train, y_train, X_test, y_test, stability_path) {
  # on garde la fréquence max sur les différents lambdas de chaque variable
  vars_max_freq <- apply(stability_path, 1, max)
  stability_indices <- seq(.6, 1, by = .05)
  names(stability_indices) <- stability_indices
  # index des variables stables pour différents seuils
  vars_idx <- sapply(stability_indices, function(s_idx) which(vars_max_freq >= s_idx))
  # scores des régressions logistiques sans pénalisation associés aux différents seuils
  scores <- sapply(vars_idx, function(v_idx) get_performance(X_train, y_train, X_test, y_test, v_idx))
  # nombre de variables sélectionnées pour les différentes seuils
  nzero <- sapply(vars_idx, length)
  # amplitude des coefficients
  ranges <- sapply(vars_idx, function(v_idx) get_coef_IQR(X_train, y_train, v_idx))
  return(data.frame(indice = stability_indices, score = scores, nzero = nzero, range = ranges, vars.idx = I(vars_idx)))
}
```

```{r, cv_func}
# retourne un data.frame donnant le score et la taille du support selon le seuil de stabilité pour n_folds modèles
cv_run_stability_selection_model <- function(X_train, y_train, lambda, n_folds = 10) {
  # assignation de chaque observation à un fold
  folds_id <- sample(rep(seq(n_folds), length = nrow(X_train)))
  cv_stability_indices_summary <- lapply(1:n_folds, function(fid) {
    whichs <- folds_id == fid
    # restriction du jeu d'entrainement aux observations n'appartenant pas au fold fid
    fold_X_train <- X_train[! whichs, ]
    fold_y_train <- y_train[! whichs]
    # et du jeu de test à celles appartenant à fid
    fold_X_test <- X_train[whichs, ]
    fold_y_test <- y_train[whichs]
    # appel à la fonction générique
    stability_path <- get_stability_path(fold_X_train, fold_y_train, lambda) # ? sample_size_coef
    return(run_stability_selection_model(fold_X_train, fold_y_train,
                                         fold_X_test, fold_y_test,
                                         stability_path))
  })
  return(do.call(rbind, cv_stability_indices_summary))
}

# à partir d'un data.frame contenant les résultats de la cross-validation retourne les indices optimaux
cv_get_optimal_stability_indices <- function(stability_indices_summary) {
  stability_indices <- unique(stability_indices_summary$indice)
  mean_sd <- lapply(stability_indices, function(sid) {
    scores <- stability_indices_summary$score[stability_indices_summary$indice == sid]
    return(data.frame(mean = mean(100 - scores), sd = sd(100 - scores)))
  })
  mean_sd <- do.call(rbind, mean_sd)
  mean_sd$indice <- stability_indices
  # indice de stabilité pour la meilleure classification
  indice_min <- max(mean_sd$indice[which(mean_sd$mean == min(mean_sd$mean))])
  # indice de stabilité pour le meilleur compromis classification / taille du support
  indice_1sd <- max(mean_sd$indice[which(mean_sd$mean <= mean_sd$mean[[indice_min]] + mean_sd$sd[[indice_min]])])
  return(list(indice.min = indice_min, indice.1sd = indice_1sd))
}
```

```{r, lasso, eval = FALSE}
lasso_mod <- cv.glmnet(X.train, y.train, type.measure = "class", family = "binomial")
lasso_lambda <- lasso_mod$lambda
lasso_pred <- predict(lasso_mod, newx = X.test, type = "class", s = lasso_mod$lambda.1se)[, 1]
lasso_vars_idx <- which(lasso_mod$glmnet.fit$beta[, which(lasso_mod$lambda == lasso_mod$lambda.1se)] != 0)
lasso_summary <- data.frame(indice = NA,
                            score = get_score(y.test, lasso_pred),
                            nzero = lasso_mod$nzero[[which(lasso_mod$lambda == lasso_mod$lambda.1se)]],
                            range = get_coef_range(X.train, y.train, lasso_vars_idx),
                            vars.idx = I(list(lasso_vars_idx)))
rownames(lasso_summary) <- "Lasso"
```

```{r, stab_sel, eval = FALSE}
stability_path <- get_stability_path(X.train, y.train, lasso_lambda)
stab_sel_summary <- run_stability_selection_model(X.train, y.train, X.test, y.test, stability_path)
rownames(stab_sel_summary) <- paste0("StabSel-", round(stab_sel_summary$indice, digits = 2))
```

```{r, cv_stab_sel, eval = FALSE}
cv_stab_sel_summary <- cv_run_stability_selection_model(X.train, y.train, lasso_lambda)
best_indices <- cv_get_optimal_stability_indices(cv_stab_sel_summary)
indice_min_idx <- which(stab_sel_summary$indice == best_indices$indice.min)
indice_1sd_idx <- which(stab_sel_summary$indice == best_indices$indice.1sd)
rownames(stab_sel_summary)[[indice_min_idx]] <- paste0(rownames(stab_sel_summary)[[indice_min_idx]], " (cv_min)")
rownames(stab_sel_summary)[[indice_1sd_idx]] <- paste0(rownames(stab_sel_summary)[[indice_1sd_idx]], " (cv_1sd)")
```

```{r, paths, eval = FALSE}
# Construction des data.frames des chemins

# De régularisation
activated_vars <- which(lasso_mod$glmnet.fit$beta != 0, arr.ind = TRUE)
reg_path <- data.frame(cbind(lasso_mod$glmnet.fit$beta[activated_vars], activated_vars))
colnames(reg_path) <- c("beta", "var", "lambda")
reg_path$lambda <- as.integer(reg_path$lambda)

# De sélection
stable_vars <- which(stability_path != 0, arr.ind = TRUE)
stab_path <- data.frame(cbind(stability_path[stable_vars], stable_vars))
colnames(stab_path) <- c("freq", "var", "lambda")
stab_path$lambda <- as.integer(stab_path$lambda)

# Jointure avec les fréquences/probabilités de sélection maximales pour la coloration
max_freq <- by(stab_path, list(stab_path$var), function(df) data.frame(var = unique(df$var), max_freq = max(df$freq)))
max_freq <- as.data.frame(do.call(rbind, max_freq))
max_freq$var <- as.factor(max_freq$var)
max_freq$max_freq <- as.numeric(max_freq$max_freq)
reg_path <- merge(reg_path, max_freq, by = "var", all.x = TRUE)
reg_path[is.na(reg_path$max_freq), "max_freq"] <- 0
stab_path <- merge(stab_path, max_freq, by = "var")

# cast en factor avec un ordre de levels precis pour que les chemins des variables les plus stables apparaissent biens
reg_path$var <- factor(reg_path$var, levels = unique(reg_path[order(reg_path$max_freq), "var"]))
stab_path$var <- factor(stab_path$var, levels = unique(stab_path[order(stab_path$max_freq), "var"]))
```

```{r, save, eval = FALSE}
save(lasso_summary, stab_sel_summary, file = "data/summaries.Rdata")
save(reg_path, stab_path, file = "data/paths.Rdata")
```

```{r, load}
load("data/summaries.Rdata")
load("data/paths.Rdata")
mods_summary <- rbind(lasso_summary, stab_sel_summary)
mods_summary$nzero <- as.integer(mods_summary$nzero)
```

```{r, nzero_score}
ggplot(data = mods_summary) +
        geom_point(aes(x = nzero, y = score, colour = rownames(mods_summary), shape = rownames(mods_summary))) +
        scale_shape_manual(name = "Modèle", values = 1:nrow(mods_summary)) +
        scale_color_discrete(name = "Modèle") +
        labs(x = "Taille du support", y = "Taux d'agrément") +
        theme_bw()
# interprétation
# Performe nettement mieux que Lasso
# Peu de variation dans les résultats de StabSel, mis à part .95 et 1 => le papier de 2010 préconise un seuil de
# stabilité entre .6 et .9 : consistant
#   => très intéressant car permet de s'affranchir du choix de lambda du Lasso qui lui à un impact fort (voir chemin de
#       régularisation)
```

```{r, range_score}
ggplot(data = mods_summary) +
        geom_point(aes(x = range, y = score, colour = rownames(mods_summary), shape = rownames(mods_summary))) +
        scale_shape_manual(name = "Modèle", values = 1:nrow(mods_summary)) +
        scale_color_discrete(name = "Modèle") +
        labs(x = "Amplitude des coefficients (IQR)", y = "Taux d'agrément") +
        theme_bw()
# interprétation :
# amplitude forte => coefficients extrêmes pour compenser les variables instables (bruits) introduites par Lasso ?
```

```{r, roc}
mods_roc <- lapply(mods_summary$vars.idx, function(vars) get_roc(X.train, y.train, X.test, y.test, vars))
names(mods_roc) <- rownames(mods_summary)
ggroc(mods_roc) +
        labs(x = "Spécificité", y = "Sensibilité") +
        scale_color_discrete(name = "Modèle") +
        theme_bw()
```

```{r, auc}
mods_auc <- data.frame(auc = sapply(mods_roc, function(mod_roc) mod_roc$auc), model = names(mods_roc))
ggplot(data = mods_auc) +
        geom_bar(aes(x = model, y = auc, fill = model), stat = "identity") +
        scale_fill_discrete(name = "Modèle") +
        labs(x = "Modèle", y = "AUC") +
        theme_bw() +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r, fig_paths}
stab_path$color <- sapply(stab_path$max_freq, function(f) min(f, .8)) # meme couleur de .8 a 1
fig_stab <- ggplot(data = stab_path) +
        geom_line(aes(x = lambda, y = freq, group = var, color = color)) +
        scale_color_gradient(low = "yellow", high = "red", limits = c(.6, .8), guide = "none") +
        labs(title = "Chemin de stabilité", x = "Lambda index", y = "Probabilité de sélection")

reg_path <- reg_path[order(reg_path$max_freq), ]
reg_path$color <- sapply(reg_path$max_freq, function(f) min(f, .8))
fig_reg <- ggplot(data = reg_path) +
        geom_line(aes(x = lambda, y = beta, group = var, color = color)) +
        scale_color_gradient(low = "yellow", high = "red", limits = c(0.6, .8), guide = "none") +
        labs(title = "Chemin de régularisation", x = "Lambda index", y = "Coefficient")
ggarrange(fig_stab, fig_reg, ncol = 2)

# de nombreuses variables stables n'apparaisent pas de le chemin de régularisation
```
